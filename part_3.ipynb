{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TWEET_DELIMITER = ' NEWTWEET '\n",
    "\n",
    "def get_tweets_texts_joined(tweets):\n",
    "    tweets_text = [tweet.full_text for tweet in tweets]\n",
    "    return TWEET_DELIMITER.join(tweets_text)\n",
    "    \n",
    "trump_tweets = api.user_timeline(screen_name = \"realDonaldTrump\",count=500, tweet_mode = 'extended')\n",
    "trump_tweets = get_tweets_texts_joined(trump_tweets)\n",
    "\n",
    "hilary_tweets = api.user_timeline(screen_name = \"HillaryClinton\",count=500, tweet_mode = 'extended')\n",
    "hilary_tweets = get_tweets_texts_joined(hilary_tweets)\n",
    "\n",
    "obama_tweets = api.user_timeline(screen_name = \"BarackObama\",count=500, tweet_mode = 'extended')\n",
    "obama_tweets = get_tweets_texts_joined(obama_tweets)\n",
    "\n",
    "seinfeld_tweets = api.user_timeline(screen_name = \"JerrySeinfeld\",count=500, tweet_mode = 'extended')\n",
    "seinfeld_tweets = get_tweets_texts_joined(seinfeld_tweets)\n",
    "\n",
    "gadot_tweets = api.user_timeline(screen_name = \"GalGadot\",count=500, tweet_mode = 'extended')\n",
    "gadot_tweets = get_tweets_texts_joined(gadot_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_tweets_updated = \" \".join(filter(lambda x: '#' not in x and '@' not in x and 'http' not in x, trump_tweets.split()))\n",
    "trump_tweets_updated\n",
    "\n",
    "hilary_tweets_updated = \" \".join(filter(lambda x: '#' not in x and '@' not in x and 'http' not in x, hilary_tweets.split()))\n",
    "hilary_tweets_updated\n",
    "\n",
    "obama_tweets_updated = \" \".join(filter(lambda x: '#' not in x and '@' not in x and 'http' not in x, obama_tweets.split()))\n",
    "obama_tweets_updated\n",
    "\n",
    "seinfeld_tweets_updated = \" \".join(filter(lambda x: '#' not in x and '@' not in x and 'http' not in x, seinfeld_tweets.split()))\n",
    "seinfeld_tweets_updated\n",
    "\n",
    "gadot_tweets_updated = \" \".join(filter(lambda x: '#' not in x and '@' not in x and 'http' not in x, gadot_tweets.split()))\n",
    "gadot_tweets_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 600\n",
    "unknown_token = \"UNKNOWNTOKEN\"\n",
    "sentence_start_token = \"SENTENCESTART\"\n",
    "sentence_end_token = \"SENTENCEEND\"\n",
    "line_break= \"NEWLINE\"\n",
    "separator= \"SEPARATOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_specific_symbols_and_clean(text):\n",
    "    text = text.replace('...', '.')\n",
    "    text = text.replace('....', '.')\n",
    "    text = text.replace('\\n',' '+ line_break + ' ')\n",
    "    text = text.replace('--',' '+ separator + ' ')\n",
    "    text = text.replace('.',' '+sentence_end_token +' '+ sentence_start_token+' ' )\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_tweets_cleaned = replace_specific_symbols_and_clean(trump_tweets_updated)\n",
    "trump_tweets_cleaned\n",
    "\n",
    "hilary_tweets_cleaned = replace_specific_symbols_and_clean(hilary_tweets_updated)\n",
    "hilary_tweets_cleaned\n",
    "\n",
    "obama_tweets_cleaned = replace_specific_symbols_and_clean(obama_tweets_updated)\n",
    "obama_tweets_cleaned\n",
    "\n",
    "seinfeld_tweets_cleaned = replace_specific_symbols_and_clean(seinfeld_tweets_updated)\n",
    "seinfeld_tweets_cleaned\n",
    "\n",
    "gadot_tweets_cleaned = replace_specific_symbols_and_clean(gadot_tweets_updated)\n",
    "gadot_tweets_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "trump_text_splitted = text_to_word_sequence(trump_tweets_cleaned, lower=False, split=\" \") #using only 10000 first words\n",
    "trump_text_splitted[0:10]\n",
    "\n",
    "hilary_text_splitted = text_to_word_sequence(hilary_tweets_cleaned, lower=False, split=\" \") #using only 10000 first words\n",
    "hilary_text_splitted[0:10]\n",
    "\n",
    "obama_text_splitted = text_to_word_sequence(obama_tweets_cleaned, lower=False, split=\" \") #using only 10000 first words\n",
    "obama_text_splitted[0:10]\n",
    "\n",
    "seinfeld_text_splitted = text_to_word_sequence(seinfeld_tweets_cleaned, lower=False, split=\" \") #using only 10000 first words\n",
    "seinfeld_text_splitted[0:10]\n",
    "\n",
    "gadot_text_splitted = text_to_word_sequence(gadot_tweets_cleaned, lower=False, split=\" \") #using only 10000 first words\n",
    "gadot_text_splitted[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "trump_tokenizer = Tokenizer(nb_words=150,char_level=False)\n",
    "trump_tokenizer.fit_on_texts(trump_text_splitted)\n",
    "\n",
    "hilary_tokenizer = Tokenizer(nb_words=150,char_level=False)\n",
    "hilary_tokenizer.fit_on_texts(hilary_text_splitted)\n",
    "\n",
    "obama_tokenizer = Tokenizer(nb_words=150,char_level=False)\n",
    "obama_tokenizer.fit_on_texts(obama_text_splitted)\n",
    "\n",
    "seinfeld_tokenizer = Tokenizer(nb_words=150,char_level=False)\n",
    "seinfeld_tokenizer.fit_on_texts(seinfeld_text_splitted)\n",
    "\n",
    "gadot_tokenizer = Tokenizer(nb_words=150,char_level=False)\n",
    "gadot_tokenizer.fit_on_texts(gadot_text_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_text_mtx = trump_tokenizer.texts_to_matrix(trump_text_splitted, mode='binary')\n",
    "\n",
    "hilary_text_mtx = hilary_tokenizer.texts_to_matrix(hilary_text_splitted, mode='binary')\n",
    "\n",
    "obama_text_mtx = obama_tokenizer.texts_to_matrix(obama_text_splitted, mode='binary')\n",
    "\n",
    "sienfeld_text_mtx = seinfeld_tokenizer.texts_to_matrix(seinfeld_text_splitted, mode='binary')\n",
    "\n",
    "gadot_text_mtx = gadot_tokenizer.texts_to_matrix(gadot_text_splitted, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_input = trump_text_mtx[:-1]\n",
    "trump_output = trump_text_mtx[1:]\n",
    "\n",
    "hilary_input = hilary_text_mtx[:-1]\n",
    "hilary_output = hilary_text_mtx[1:]\n",
    "\n",
    "obama_input = obama_text_mtx[:-1]\n",
    "obama_output = obama_text_mtx[1:]\n",
    "\n",
    "seinfeld_input = sienfeld_text_mtx[:-1]\n",
    "seinfeld_output = sienfeld_text_mtx[1:]\n",
    "\n",
    "gadot_input = gadot_text_mtx[:-1]\n",
    "gadot_output = gadot_text_mtx[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "trump_vocab = pd.DataFrame({'word':trump_text_splitted,'code':np.argmax(trump_text_mtx,axis=1)})\n",
    "\n",
    "hilary_vocab = pd.DataFrame({'word':hilary_text_splitted,'code':np.argmax(hilary_text_mtx,axis=1)})\n",
    "\n",
    "obama_vocab = pd.DataFrame({'word':obama_text_splitted,'code':np.argmax(obama_text_mtx,axis=1)})\n",
    "\n",
    "seinfeld_vocab = pd.DataFrame({'word':seinfeld_text_splitted,'code':np.argmax(sienfeld_text_mtx,axis=1)})\n",
    "\n",
    "gadot_vocab = pd.DataFrame({'word':gadot_text_splitted,'code':np.argmax(gadot_text_mtx,axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_model = Sequential()\n",
    "\n",
    "hilary_model = Sequential()\n",
    "\n",
    "obama_model = Sequential()\n",
    "\n",
    "seinfeld_model = Sequential()\n",
    "\n",
    "gadot_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_model.add(Embedding(input_dim=trump_input.shape[1],output_dim= 42, input_length=trump_input.shape[1]))\n",
    "\n",
    "hilary_model.add(Embedding(input_dim=hilary_input.shape[1],output_dim= 42, input_length=hilary_input.shape[1]))\n",
    "\n",
    "obama_model.add(Embedding(input_dim=obama_input.shape[1],output_dim= 42, input_length=obama_input.shape[1]))\n",
    "\n",
    "seinfeld_model.add(Embedding(input_dim=seinfeld_input.shape[1],output_dim= 42, input_length=seinfeld_input.shape[1]))\n",
    "\n",
    "gadot_model.add(Embedding(input_dim=gadot_input.shape[1],output_dim= 42, input_length=gadot_input.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_model.add(Flatten())\n",
    "trump_model.add(Dense(trump_output.shape[1], activation='sigmoid'))\n",
    "\n",
    "hilary_model.add(Flatten())\n",
    "hilary_model.add(Dense(hilary_output.shape[1], activation='sigmoid'))\n",
    "\n",
    "obama_model.add(Flatten())\n",
    "obama_model.add(Dense(obama_output.shape[1], activation='sigmoid'))\n",
    "\n",
    "seinfeld_model.add(Flatten())\n",
    "seinfeld_model.add(Dense(seinfeld_output.shape[1], activation='sigmoid'))\n",
    "\n",
    "gadot_model.add(Flatten())\n",
    "gadot_model.add(Dense(gadot_output.shape[1], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_model.summary(line_length=100)\n",
    "\n",
    "hilary_model.summary(line_length=100)\n",
    "\n",
    "obama_model.summary(line_length=100)\n",
    "\n",
    "seinfeld_model.summary(line_length=100)\n",
    "\n",
    "gadot_model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "hilary_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "obama_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "seinfeld_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "gadot_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_model.fit(trump_input, y=trump_output, batch_size=200, nb_epoch=50, verbose=1, validation_split=0.2)\n",
    "\n",
    "hilary_model.fit(hilary_input, y=hilary_output, batch_size=200, nb_epoch=50, verbose=1, validation_split=0.2)\n",
    "\n",
    "obama_model.fit(obama_input, y=obama_output, batch_size=200, nb_epoch=50, verbose=1, validation_split=0.2)\n",
    "\n",
    "seinfeld_model.fit(seinfeld_input, y=seinfeld_output, batch_size=200, nb_epoch=50, verbose=1, validation_split=0.2)\n",
    "\n",
    "gadot_model.fit(gadot_input, y=gadot_output, batch_size=200, nb_epoch=50, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_score = trump_model.evaluate(trump_input,trump_output, verbose=0)\n",
    "print('trump score : ', trump_score)\n",
    "\n",
    "hilary_score = hilary_model.evaluate(hilary_input,hilary_output, verbose=0)\n",
    "print('hilary score : ', hilary_score)\n",
    "\n",
    "obama_score = obama_model.evaluate(obama_input,obama_output, verbose=0)\n",
    "print('obama score : ', obama_score)\n",
    "\n",
    "seinfeld_score = seinfeld_model.evaluate(seinfeld_input,seinfeld_output, verbose=0)\n",
    "print('seinfeld score : ', seinfeld_score)\n",
    "\n",
    "gadot_score = gadot_model.evaluate(gadot_input,gadot_output, verbose=0)\n",
    "print('gadot score : ', gadot_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(text,token,model,vocabulary):\n",
    "    '''Predicts the following word, given a text word, a tokenizer to convert it to 1-hot vector, a trained model and a vocabulary\n",
    "    with word and index representations'''\n",
    "    #converting the word to 1-hot matrix represenation\n",
    "    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n",
    "    tmp = token.texts_to_matrix(tmp, mode='binary')\n",
    "    #predicting next word\n",
    "    p = model.predict(tmp)[0]\n",
    "    match = find_random_word_index(p)\n",
    "#     print(str(match))\n",
    "#     print(vocabulary[match])\n",
    "    return vocabulary[vocabulary['code']==match]['word'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def find_random_word_index(v):\n",
    "    found = False\n",
    "    while found == False:\n",
    "        index_rand_choice = random.randint(0, len(v) - 1)\n",
    "        if v[index_rand_choice] != 0:\n",
    "            return index_rand_choice\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sentence(token, model, vocab):\n",
    "    prev_word = 'NEWTWEET'\n",
    "    next_word = ''\n",
    "    res = '';\n",
    "    count_words = 0\n",
    "    while next_word.strip() != 'NEWTWEET' and count_words < 30:\n",
    "        next_word = get_next(prev_word, token, model, vocab)\n",
    "        res = res + \" \" + next_word\n",
    "        prev_word = next_word\n",
    "        count_words += 1\n",
    "    return res.rsplit(' ', 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(get_next('Democrats', trump_tokenizer, trump_model, trump_vocab))\n",
    "\n",
    "print(get_next('Democrats', hilary_tokenizer, hilary_model, hilary_vocab))\n",
    "\n",
    "print(get_next('Democrats', obama_tokenizer, obama_model, obama_vocab))\n",
    "\n",
    "print(get_next('Democrats', seinfeld_tokenizer, seinfeld_model, seinfeld_vocab))\n",
    "\n",
    "print(get_next('Democrats', gadot_tokenizer, gadot_model, gadot_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(get_next('Democrats', trump_tokenizer, trump_model, trump_vocab))\n",
    "\n",
    "print(get_next('Democrats', hilary_tokenizer, hilary_model, hilary_vocab))\n",
    "\n",
    "print(get_next('Democrats', obama_tokenizer, obama_model, obama_vocab))\n",
    "\n",
    "print(get_next('Democrats', seinfeld_tokenizer, seinfeld_model, seinfeld_vocab))\n",
    "\n",
    "print(get_next('Democrats', gadot_tokenizer, gadot_model, gadot_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_trump_tweets = []\n",
    "all_hilary_tweets = []\n",
    "all_obama_tweets = []\n",
    "all_seinfeld_tweets = []\n",
    "all_gadot_tweets = []\n",
    "actual_tweeters = []\n",
    "\n",
    "for i in range(0, 150):\n",
    "    trump_tweet = create_sentence(trump_tokenizer, trump_model, trump_vocab)\n",
    "    trump_tweet_after_stem = stem_sentence(trump_tweet)\n",
    "    all_trump_tweets.extend([trump_tweet_after_stem])\n",
    "    actual_tweeters.extend(['Donald J. Trump'])\n",
    "\n",
    "for i in range(0, 150):\n",
    "    hilary_tweet = create_sentence(hilary_tokenizer, hilary_model, hilary_vocab)\n",
    "    hilary_tweet_after_stem = stem_sentence(hilary_tweet)\n",
    "    all_hilary_tweets.extend([hilary_tweet_after_stem])\n",
    "    actual_tweeters.extend(['Hillary Clinton'])\n",
    "    \n",
    "for i in range(0, 150):\n",
    "    obama_tweet = create_sentence(obama_tokenizer, obama_model, obama_vocab)\n",
    "    obama_tweet_after_stem = stem_sentence(obama_tweet)\n",
    "    all_obama_tweets.extend([obama_tweet_after_stem])\n",
    "    actual_tweeters.extend(['Barack Obama'])\n",
    "\n",
    "for i in range(0, 150):\n",
    "    seinfeld_tweet = create_sentence(seinfeld_tokenizer, seinfeld_model, seinfeld_vocab)\n",
    "    seinfeld_tweet_after_stem = stem_sentence(seinfeld_tweet)\n",
    "    all_seinfeld_tweets.extend([seinfeld_tweet_after_stem])\n",
    "    actual_tweeters.extend(['Jerry Seinfeld'])\n",
    "\n",
    "for i in range(0, 150):\n",
    "    gadot_tweet = create_sentence(gadot_tokenizer, gadot_model, gadot_vocab)\n",
    "    gadot_tweet_after_stem = stem_sentence(gadot_tweet)\n",
    "    all_gadot_tweets.extend([gadot_tweet_after_stem])\n",
    "    actual_tweeters.extend(['Gal Gadot'])\n",
    "\n",
    "\n",
    "all_trump_tweets.extend(alltweets_text)\n",
    "all_hilary_tweets.extend(alltweets_text)\n",
    "all_obama_tweets.extend(alltweets_text)\n",
    "all_seinfeld_tweets.extend(alltweets_text)\n",
    "all_gadot_tweets.extend(alltweets_text)\n",
    "all_trump_tweets_BOW = create_bag_of_words(all_trump_tweets)\n",
    "all_hilary_tweets_BOW = create_bag_of_words(all_hilary_tweets)\n",
    "all_obama_tweets_BOW = create_bag_of_words(all_obama_tweets)\n",
    "all_seinfeld_tweets_BOW = create_bag_of_words(all_seinfeld_tweets)\n",
    "all_gadot_tweets_BOW = create_bag_of_words(all_gadot_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
